{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSL+GA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "EuLK7FJhhuGV",
        "qK3l4ke5dEEW",
        "1snZ2PQgLB71",
        "974ypQUhLKL2",
        "0ch9VUqBj4oA"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wipWgYhhoQd"
      },
      "source": [
        "#SSL+GA\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr41t6ngoZIP"
      },
      "source": [
        "現時点での使い方\n",
        "\n",
        "・encoder生成\n",
        "\n",
        "　　import section\n",
        "\n",
        "　　CL内\n",
        "\n",
        "　  DataUtils,OtherUtils,Network,Pretrain\n",
        "\n",
        "　　の順で実行することで作成できます\n",
        "\n",
        "\\\n",
        "\n",
        "・GA使用法\n",
        "\n",
        "　　encoderがすでに生成されていれば\n",
        "\n",
        "　　Pretrain以外順に実行すればできます\n",
        "\n",
        "　　そのため現在Pre trainの実行部はコメントアウトしており\n",
        "\n",
        "　　import section,CL,GA part で実行できます\n",
        "\n",
        "\\\n",
        "\n",
        "参考資料\n",
        "\n",
        "RAdam:https://github.com/LiyuanLucasLiu/RAdam\n",
        "\n",
        "SimCLR:https://github.com/leftthomas/SimCLR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuLK7FJhhuGV"
      },
      "source": [
        "#import section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MJifYpHuRw3"
      },
      "source": [
        "#gpu 確認\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cBYSEmaSwNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8766ca-20cb-4102-9199-55568e619cbe"
      },
      "source": [
        "#drive にデータ保存用\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N9zkOCKDHz-"
      },
      "source": [
        "#https://github.com/LeeDoYup/FixMatch-pytorch/tree/main/datasets\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import sampler, DataLoader,Dataset\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "import torch.distributed as dist\n",
        "import numpy as np\n",
        "\n",
        "import copy\n",
        "\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.models as models\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import os\n",
        "import contextlib\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import os\n",
        "\n",
        "import logging\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
        "from PIL import Image\n",
        "\n",
        "import argparse\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from thop import profile, clever_format\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.resnet import resnet18\n",
        "from torchvision.models.resnet import resnet50\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import csv\n",
        "\n",
        "from deap import base, creator,tools\n",
        "import shutil\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F72EPEOjWoio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed92fab-c15a-4618-a345-879e1ca22192"
      },
      "source": [
        "#dset = getattr(torchvision.datasets,\"CIFAR10\")\n",
        "#dset = dset(\"./data\", train=True,download=True)\n",
        "#data, targets = np.array(dset.data), np.array(dset.targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJBGNo01K3hg"
      },
      "source": [
        "#Contrastive Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF9iUYcVK8-R"
      },
      "source": [
        "##Data Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqbr3MzZNL2D"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
        "\n",
        "valid_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
        "\n",
        "#\n",
        "class CIFAR10Pair(CIFAR10):\n",
        "    \"\"\"CIFAR10 Dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            pos_1 = self.transform(img)\n",
        "            pos_2 = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return pos_1, pos_2, target\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPKTC_G9TTyn"
      },
      "source": [
        "\"\"\"\n",
        "ラベル分割\n",
        "(data,target):(入力データ,ラベル)\n",
        "num_labels:ラベル付きデータの枚数\n",
        "num_classes:ラベル数\n",
        "index:ラベル付きデータが確定しているときのインデックス\n",
        "include_lb_to_ulb:ラベルなしにもラベル付き入力データを含ませるか否か\n",
        "\"\"\"\n",
        "def split_ga_data(data, target, num_labels, num_classes, index=None, include_lb_to_ulb=True):\n",
        "    data, target = np.array(data), np.array(target)\n",
        "    lb_data, lbs, lb_idx = sample_labeled_data(data, target, num_labels, num_classes, index)\n",
        "    ulb_idx = np.array(sorted(list(set(range(len(data))) - set(lb_idx)))) #unlabeled_data index of data\n",
        "    if include_lb_to_ulb:\n",
        "        return lb_data, lbs, data, target\n",
        "    else:\n",
        "        return lb_data, lbs, data[ulb_idx], target[ulb_idx]\n",
        "    \n",
        "    \n",
        "\"\"\"\n",
        "データ選出\n",
        "(data,target):(入力データ,ラベル)\n",
        "num_labels:ラベル付きデータの枚数\n",
        "num_classes:ラベル数\n",
        "index:ラベル付きデータが確定しているときのインデックス\n",
        "isUniform:ラベルに対するデータを均一にとるか否か\n",
        "\"\"\"\n",
        "def sample_labeled_data(data, target, \n",
        "                         num_labels,\n",
        "                         num_classes,\n",
        "                         index=None,\n",
        "                        isUniform=True):\n",
        "    #assert num_labels % num_classes == 0\n",
        "    if not index is None:\n",
        "        index = np.array(index, dtype=np.int32)\n",
        "        return data[index], target[index], index\n",
        "    \n",
        "    samples_per_class = int(num_labels / num_classes)\n",
        "    \n",
        "    lb_data = []\n",
        "    lbs = []\n",
        "    lb_idx = []\n",
        "    if isUniform:\n",
        "        for c in range(num_classes):\n",
        "            idx = np.where(target == c)[0]\n",
        "            idx = np.random.choice(idx, samples_per_class, False)\n",
        "            lb_idx.extend(idx)\n",
        "            lb_data.extend(data[idx])\n",
        "            lbs.extend(target[idx])\n",
        "    else:\n",
        "        idx = np.random.choice(target, num_labels, False)\n",
        "        lb_idx.extend(idx)\n",
        "        lb_data.extend(data[idx])\n",
        "        lbs.extend(target[idx])\n",
        "        \n",
        "    return np.array(lb_data), np.array(lbs), np.array(lb_idx)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-WGbA5vSflj"
      },
      "source": [
        "\"\"\"\n",
        "基本のデータセット\n",
        "\"\"\"\n",
        "class BasicDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 data,\n",
        "                 targets=None,\n",
        "                 num_classes=None,\n",
        "                 transform=None,\n",
        "                 onehot=False,\n",
        "                 *args, **kwargs):\n",
        "\n",
        "        super(BasicDataset, self).__init__()\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        self.onehot = onehot\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.target_transform = None\n",
        "                \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        return self.transform(img), target\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiNb4KJUzeJU"
      },
      "source": [
        "\"\"\"\n",
        "GA用データセット\n",
        "\"\"\"\n",
        "class GA_Dataset:\n",
        "    \n",
        "    def __init__(self,\n",
        "                 name='cifar10',\n",
        "                 train=True,\n",
        "                 num_classes=10,\n",
        "                 data_dir='./data'):\n",
        "        \n",
        "        self.name = name\n",
        "        self.train = train\n",
        "        self.data_dir = data_dir\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        if self.train is True:\n",
        "            self.transform = train_transform\n",
        "        else:\n",
        "            self.transform = test_transform\n",
        "        \n",
        "    def get_data(self):\n",
        "        \n",
        "        dset = getattr(torchvision.datasets, self.name.upper())\n",
        "        dset = dset(self.data_dir, train=self.train, download=True)\n",
        "        data, targets = dset.data, dset.targets\n",
        "        return data, targets\n",
        "    \n",
        "    \n",
        "    def get_dset(self, use_strong_transform=False, \n",
        "                 strong_transform=None, onehot=False):\n",
        "        \n",
        "        data, targets = self.get_data()\n",
        "        num_classes = self.num_classes\n",
        "        transform = self.transform\n",
        "        data_dir = self.data_dir\n",
        "        \n",
        "        return BasicDataset(data, targets, num_classes, transform, onehot)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4qoVAvKSO82"
      },
      "source": [
        "\"\"\"\n",
        "データ生成(正確にはインデックスに応じた分割)\n",
        "train_index:ラベル付きデータのインデックス\n",
        "temp_index:ラベルをつけたいデータのインデックス\n",
        "temp_target:GAの1個体であるラベル集合\n",
        "eval_index:テストデータのインデックス\n",
        "\"\"\"\n",
        "def creat_dset(train_index,temp_index,temp_target,eval_index,\n",
        "               name='cifar10',num_classes=10,data_dir='./data',train=True):\n",
        "    dset = getattr(torchvision.datasets, name.upper())\n",
        "    dset = dset(data_dir, train=train, download=False)\n",
        "    data, targets = np.array(dset.data), np.array(dset.targets)\n",
        "\n",
        "    lb_data, lbs, lb_idx = sample_labeled_data(data, targets,\n",
        "                                               len(train_index), num_classes, train_index)\n",
        "    if len(temp_target) != 0:\n",
        "      te_data, tes, te_idx = sample_labeled_data(data, targets, \n",
        "                                               len(temp_index), num_classes, temp_index)\n",
        "      lb_data = np.concatenate([lb_data, np.array(te_data)])\n",
        "      lbs = np.concatenate([lbs,np.array(temp_target)])\n",
        "\n",
        "    ev_data, evs, ev_idx = sample_labeled_data(data, targets, \n",
        "                                               len(eval_index), num_classes, eval_index)\n",
        "    return lb_data, lbs, ev_data, evs\n",
        "\"\"\"\n",
        "データセット生成\n",
        "\"\"\"\n",
        "def creat_GA_Dataset(train_index,temp_index,temp_target,eval_index,\n",
        "                      name='cifar10',num_classes=10,data_dir='./data'):\n",
        "    lb_data,lb_targets,ev_data,ev_targets = creat_dset(train_index,temp_index,temp_target,eval_index,\n",
        "                                                                          name,num_classes,data_dir,train=True)\n",
        "    onehot =False\n",
        "    lb_dset = BasicDataset(lb_data, lb_targets, num_classes, \n",
        "                               train_transform, onehot)\n",
        "    ev_dset = BasicDataset(ev_data, ev_targets, num_classes,\n",
        "                           valid_transform, onehot)\n",
        "    \n",
        "    return lb_dset,  ev_dset\n",
        "\n",
        "\"\"\"\n",
        "上記に加え\n",
        "train dataにランダムなラベルが付与せれたダミーデータを追加\n",
        "\"\"\"\n",
        "def creat_pDMdset(train_index,temp_index,temp_target,eval_index,dum_index,dum_target,\n",
        "               name='cifar10',num_classes=10,data_dir='./data',train=True):\n",
        "    dset = getattr(torchvision.datasets, name.upper())\n",
        "    dset = dset(data_dir, train=train, download=False)\n",
        "    data, targets = np.array(dset.data), np.array(dset.targets)\n",
        "\n",
        "    lb_data, lbs, lb_idx = sample_labeled_data(data, targets,\n",
        "                                               len(train_index), num_classes, train_index)\n",
        "    if len(temp_target) != 0:\n",
        "      te_data, tes, te_idx = sample_labeled_data(data, targets, \n",
        "                                               len(temp_index), num_classes, temp_index)\n",
        "      lb_data = np.concatenate([lb_data, np.array(te_data)])\n",
        "      lbs = np.concatenate([lbs,np.array(temp_target)])\n",
        "    \n",
        "    if len(dum_target) != 0:\n",
        "      dum_data, dums, dum_idx = sample_labeled_data(data, targets, \n",
        "                                               len(dum_index), num_classes, dum_index)\n",
        "      lb_data = np.concatenate([lb_data, np.array(dum_data)])\n",
        "      lbs = np.concatenate([lbs,np.array(dum_target)])\n",
        "\n",
        "    ev_data, evs, ev_idx = sample_labeled_data(data, targets, \n",
        "                                               len(eval_index), num_classes, eval_index)\n",
        "    return lb_data, lbs, ev_data, evs\n",
        "\n",
        "def creat_GADM_Dataset(train_index,temp_index,temp_target,eval_index,dum_index=None,dum_target=None,\n",
        "                      name='cifar10',num_classes=10,data_dir='./data'):\n",
        "    lb_data,lb_targets,ev_data,ev_targets = creat_pDMdset(train_index,temp_index,temp_target,eval_index,\n",
        "                                                         dum_index,dum_target,name,num_classes,data_dir,train=True)\n",
        "    #train_trans = get_transform(mean[name], std[name], train=True)\n",
        "    #eval_trans = get_transform(mean[name], std[name], train=False)\n",
        "\n",
        "    onehot =False\n",
        "    lb_dset = BasicDataset(lb_data, lb_targets, num_classes, \n",
        "                               train_transform, onehot)\n",
        "    ev_dset = BasicDataset(ev_data, ev_targets, num_classes,\n",
        "                           valid_transform, onehot)\n",
        "    \n",
        "    return lb_dset,  ev_dset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK3l4ke5dEEW"
      },
      "source": [
        "##Other Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTBCRpzVdC5p"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "\"\"\"\n",
        "RAdamというoptimizer\n",
        "初期学習に自動化されたWarmUpSGD、途中からAdam\n",
        "のようなイメージ\n",
        "引用元:https://github.com/LiyuanLucasLiu/RAdam\n",
        "\"\"\"\n",
        "class RAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        \n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\n",
        "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
        "            for param in params:\n",
        "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
        "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = group['buffer'][int(state['step'] % 10)]\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    # more conservative since it's an approximated value\n",
        "                    if N_sma >= 5:\n",
        "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    elif self.degenerated_to_sgd:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = -1\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "                elif step_size > 0:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEkznTRPaieD"
      },
      "source": [
        "\"\"\"\n",
        "WarmUpつきcosスケジューラ\n",
        "\"\"\"\n",
        "def get_lr(step, total_steps, lr_max, lr_min):\n",
        "    \"\"\"Compute learning rate according to cosine annealing schedule.\"\"\"\n",
        "    return lr_min + (lr_max - lr_min) * 0.5 * (1 + np.cos(step / total_steps * np.pi))\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer,\n",
        "                                    num_training_steps,\n",
        "                                    num_cycles=7./16.,\n",
        "                                    num_warmup_steps=0,\n",
        "                                    last_epoch=-1):\n",
        "    '''\n",
        "    Get cosine scheduler (LambdaLR).\n",
        "    if warmup is needed, set num_warmup_steps (int) > 0.\n",
        "    '''\n",
        "    \n",
        "    def _lr_lambda(current_step):\n",
        "        '''\n",
        "        _lr_lambda returns a multiplicative factor given an interger parameter epochs.\n",
        "        Decaying criteria: last_epoch\n",
        "        '''\n",
        "        \n",
        "        if current_step < num_warmup_steps:\n",
        "            _lr = float(current_step) / float(max(1, num_warmup_steps))\n",
        "        else:\n",
        "            num_cos_steps = float(current_step - num_warmup_steps)\n",
        "            num_cos_steps = num_cos_steps / float(max(1, num_training_steps - num_warmup_steps))\n",
        "            _lr = max(0.0, math.cos(math.pi * num_cycles * num_cos_steps))\n",
        "        return _lr\n",
        "    \n",
        "    return LambdaLR(optimizer, _lr_lambda, last_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1snZ2PQgLB71"
      },
      "source": [
        "##Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yTcXpyFNMoI"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "事前学習(encoder + projection head)\n",
        "\"\"\"\n",
        "class Model(nn.Module):\n",
        "    def __init__(self,base_encoder=resnet18(pretrained=False), projection_dim=128):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.f = []\n",
        "        for name, module in resnet18().named_children():\n",
        "            if name == 'conv1':\n",
        "                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "            if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d):\n",
        "                self.f.append(module)\n",
        "        # encoder\n",
        "        self.f = nn.Sequential(*self.f)\n",
        "\n",
        "        # projection head\n",
        "        self.g = nn.Sequential(nn.Linear(512, 1024, bias=False), nn.BatchNorm1d(1024),\n",
        "                               nn.ReLU(inplace=True), nn.Linear(1024, projection_dim, bias=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(x)\n",
        "        feature = torch.flatten(x, start_dim=1)\n",
        "        out = self.g(feature)\n",
        "        return F.normalize(feature, dim=-1), F.normalize(out, dim=-1)\n",
        "\n",
        "\"\"\"\n",
        "分類器学習(encoder + classifier)\n",
        "encoder の出力次元とclassifierの入力次元をあわせること\n",
        "\"\"\"\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained_path):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.f = Model().f\n",
        "        # classifier\n",
        "        self.fc = nn.Linear(512, num_classes, bias=True)\n",
        "        self.load_state_dict(torch.load(pretrained_path, map_location='cpu'), strict=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(x)\n",
        "        feature = torch.flatten(x, start_dim=1)\n",
        "        out = self.fc(feature)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rr86A5ILFam"
      },
      "source": [
        "##Pre Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q8LFpcGNNJb"
      },
      "source": [
        "\"\"\"\n",
        "encoderの学習クラス\n",
        "net:ネットワーク\n",
        "feature_dim:出力次元\n",
        "temperature:Contrastive Learning(: CL)の温度係数\n",
        "batch_size,epochs:バッチサイズ,エポック数\n",
        "save_dir:保存フォルダ\n",
        "\n",
        "k,cについて\n",
        "test dataに対してtrain dataのc枚におけるfeatureが似ているものをk枚取り出して\n",
        "それらのラベルの割合からtest dataのラベルを推定してtestの識別率の数値を出している\n",
        "\n",
        "ラベル数が限られている実験なので不必要\n",
        "一応残しているが消したほうがいいかも\n",
        "\"\"\"\n",
        "class  CL_Train:\n",
        "    def __init__(self, net, feature_dim, temperature, k, batch_size, epochs, save_dir, c):\n",
        "\n",
        "        super(CL_Train, self).__init__()\n",
        "\n",
        "        self.net = net\n",
        "        self.feature_dim = feature_dim\n",
        "        self.temperature = temperature\n",
        "        self.k = k\n",
        "        self.c = c\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.optimizer = None\n",
        "        self.loader_dict = {}\n",
        "\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.it = 0\n",
        "        self.best_acc = 0\n",
        "\n",
        "\n",
        "    def set_data_loader(self, loader_dict):\n",
        "        self.loader_dict =loader_dict\n",
        "    \n",
        "    def set_optimizer(self, optimizer, scheduler=None):\n",
        "        self.optimizer = optimizer\n",
        "        if scheduler is not None:\n",
        "            self.scheduler = scheduler\n",
        "        else:\n",
        "            self.scheduler = None\n",
        "\n",
        "    def train_1epoch(self):\n",
        "        self.net.train()\n",
        "        total_loss, total_num = 0.0, 0\n",
        "        for pos_1, pos_2, target in self.loader_dict['train']:\n",
        "            pos_1, pos_2 = pos_1.cuda(non_blocking=True), pos_2.cuda(non_blocking=True)\n",
        "            feature_1, out_1 = self.net(pos_1)\n",
        "            feature_2, out_2 = self.net(pos_2)\n",
        "            # [2*B, D]\n",
        "            out = torch.cat([out_1, out_2], dim=0)\n",
        "            # [2*B, 2*B]\n",
        "            sim_matrix = torch.exp(torch.mm(out, out.t().contiguous()) / self.temperature)\n",
        "            mask = (torch.ones_like(sim_matrix) - torch.eye(2 * self.batch_size, device=sim_matrix.device)).bool()\n",
        "            # [2*B, 2*B-1]\n",
        "            sim_matrix = sim_matrix.masked_select(mask).view(2 * self.batch_size, -1)\n",
        "\n",
        "            # compute loss\n",
        "            pos_sim = torch.exp(torch.sum(out_1 * out_2, dim=-1) / self.temperature)\n",
        "            # [2*B]\n",
        "            pos_sim = torch.cat([pos_sim, pos_sim], dim=0)\n",
        "            loss = (- torch.log(pos_sim / sim_matrix.sum(dim=-1))).mean()\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            if self.scheduler:\n",
        "                self.scheduler.step()\n",
        "\n",
        "            total_num += self.batch_size\n",
        "            total_loss += loss.item() * self.batch_size\n",
        "\n",
        "        return total_loss / total_num\n",
        "\n",
        "\n",
        "    def test(self):\n",
        "        self.net.eval()\n",
        "        total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n",
        "        with torch.no_grad():\n",
        "            # generate feature bank\n",
        "            for data, _, target in self.loader_dict['memory']:\n",
        "                feature, out = self.net(data.cuda(non_blocking=True))\n",
        "                feature_bank.append(feature)\n",
        "            # [D, N]\n",
        "            feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n",
        "            # [N]\n",
        "            feature_labels = torch.tensor(self.loader_dict['memory'].dataset.targets, device=feature_bank.device)\n",
        "            \n",
        "            for data, _, target in self.loader_dict['test']:\n",
        "                data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "                feature, out = self.net(data)\n",
        "\n",
        "                total_num += data.size(0)\n",
        "                # compute cos similarity between each feature vector and feature bank ---> [B, N]\n",
        "                sim_matrix = torch.mm(feature, feature_bank)\n",
        "                # [B, K]\n",
        "                sim_weight, sim_indices = sim_matrix.topk(k=self.k, dim=-1)\n",
        "                # [B, K]\n",
        "                sim_labels = torch.gather(feature_labels.expand(data.size(0), -1), dim=-1, index=sim_indices)\n",
        "                sim_weight = (sim_weight / self.temperature).exp()\n",
        "\n",
        "                # counts for each class\n",
        "                one_hot_label = torch.zeros(data.size(0) * self.k, self.c, device=sim_labels.device)\n",
        "                # [B*K, C]\n",
        "                one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n",
        "                # weighted score ---> [B, C]\n",
        "                pred_scores = torch.sum(one_hot_label.view(data.size(0), -1, self.c) * sim_weight.unsqueeze(dim=-1), dim=1)\n",
        "\n",
        "                pred_labels = pred_scores.argsort(dim=-1, descending=True)\n",
        "                total_top1 += torch.sum((pred_labels[:, :1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "                total_top5 += torch.sum((pred_labels[:, :5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "\n",
        "        return total_top1 / total_num, total_top5 / total_num\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        results = {'train_loss': [], 'test_acc@1': [], 'test_acc@5': []}\n",
        "        save_name_pre = '{}_{}_{}_{}_{}'.format(self.feature_dim, self.temperature,\\\n",
        "                                                self.k, self.batch_size, self.epochs)\n",
        "        os.makedirs(self.save_dir,exist_ok=True)\n",
        "        if self.it is 0:\n",
        "            with open('{}{}_statistics.csv'.format(self.save_dir,save_name_pre), 'w') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow(['epoch', 'train_loss', 'test_acc@1', 'test_acc@5'])\n",
        "        best_acc = self.best_acc\n",
        "        start = self.it + 1\n",
        "        \n",
        "        print('[{}]//Start Epoch:{}'.format(datetime.datetime.now(), start))\n",
        "        for epoch in range(start, self.epochs + 1):\n",
        "            train_loss = self.train_1epoch()\n",
        "            results['train_loss'].append(train_loss)\n",
        "            test_acc_1, test_acc_5 = self.test()\n",
        "            results['test_acc@1'].append(test_acc_1)\n",
        "            results['test_acc@5'].append(test_acc_5)\n",
        "            print('[{}]//Test Epoch: [{}/{}] Loss:{:.2f} Acc@1:{:.2f} Acc@5:{:.2f}'\n",
        "                                     .format(datetime.datetime.now(), epoch, self.epochs, \\\n",
        "                                             train_loss, test_acc_1, test_acc_5))\n",
        "            #data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n",
        "            #data_frame.to_csv('{}{}_statistics.csv'.format(self.save_dir,save_name_pre), index_label='epoch')\n",
        "            self.it = epoch\n",
        "            if test_acc_1 >= best_acc:\n",
        "                best_acc = test_acc_1\n",
        "                torch.save(self.net.state_dict(), '{}best_model.pth'.format(args.save_dir))\n",
        "                print('best model saved')\n",
        "            self.save_state( 'state.pth', args.save_dir,best_acc)\n",
        "            torch.save(self.net.state_dict(), '{}latest_model.pth'.format(args.save_dir))\n",
        "            with open('{}{}_statistics.csv'.format(self.save_dir,save_name_pre), 'a') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([epoch, train_loss, test_acc_1, test_acc_5])\n",
        "\n",
        "\n",
        "    def save_model(self, save_name, save_path):\n",
        "        save_filename = os.path.join(save_path, save_name)\n",
        "        torch.save({'net':self.net.state_dict(),\n",
        "                    'optimizer': self.optimizer.state_dict(), #'scheduler':self.scheduler.state_dict(),\n",
        "                    'it': self.it}, save_filename)\n",
        "        \n",
        "    def save_state(self, save_name, save_path,best_acc):\n",
        "        save_filename = os.path.join(save_path, save_name)\n",
        "        if self.scheduler is None:\n",
        "            torch.save({'best_acc':best_acc,\n",
        "                      'optimizer': self.optimizer.state_dict(),\n",
        "                      'it': self.it}, save_filename)\n",
        "        else:\n",
        "            torch.save({'best_acc':best_acc,\n",
        "                      'optimizer': self.optimizer.state_dict(),\n",
        "                      'scheduler':self.scheduler.state_dict(),\n",
        "                      'it': self.it}, save_filename)\n",
        "    \n",
        "    \n",
        "    def load_model(self, load_path):\n",
        "        checkpoint = torch.load(load_path)\n",
        "\n",
        "        for key in checkpoint.keys():\n",
        "            #if hasattr(self, key) and getattr(self, key) is not None:\n",
        "            if 'net' in key:\n",
        "                self.net.load_state_dict(checkpoint[key], strict=False)\n",
        "            elif 'it' in key:\n",
        "                #self.it = 0\n",
        "                self.it = checkpoint[key]\n",
        "            elif 'optimizer' in key:\n",
        "                self.optimizer.load_state_dict(checkpoint[key])\n",
        "            elif 'scheduler' in key:\n",
        "                self.scheduler.load_state_dict(checkpoint[key])\n",
        "            elif 'best_acc'  in  key:\n",
        "                self.best_acc  =checkpoint[key]\n",
        "            else:\n",
        "                getattr(self, key).load_state_dict(checkpoint[key])\n",
        "            print(f\"Check Point Loading: {key} is LOADED\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGGV1zqt4kBI"
      },
      "source": [
        "\"\"\"\n",
        "事前学習を実行\n",
        "\"\"\"\n",
        "def CL_main(args):\n",
        "    \n",
        "    train_data = CIFAR10Pair(root='data', train=True, transform=train_transform, download=True)\n",
        "    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=1, pin_memory=True,\n",
        "                              drop_last=True)\n",
        "    memory_data = CIFAR10Pair(root='data', train=True, transform=test_transform, download=True)\n",
        "    memory_loader = DataLoader(memory_data, batch_size=args.batch_size, shuffle=False, num_workers=1, pin_memory=True)\n",
        "    test_data = CIFAR10Pair(root='data', train=False, transform=test_transform, download=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False, num_workers=1, pin_memory=True)\n",
        "    loader_dict = {'train':train_loader,'memory':memory_loader,'test':test_loader}\n",
        "\n",
        "    model = Model(projection_dim=args.projection_dim).cuda()\n",
        "    flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).cuda(),))\n",
        "    flops, params = clever_format([flops, params])\n",
        "    print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
        "    c = len(memory_data.classes)\n",
        "\n",
        "    trainer = CL_Train(model,\n",
        "                       args.projection_dim,\n",
        "                       args.temperature,\n",
        "                       args.k,\n",
        "                       args.batch_size,\n",
        "                       args.epochs,\n",
        "                       args.save_dir,\n",
        "                       c)\n",
        "    optimizer = RAdam(params=model.parameters(), lr=args.lr)\n",
        "    scheduler = LambdaLR(\n",
        "        optimizer,\n",
        "        lr_lambda=lambda step: get_lr(  \n",
        "            step,\n",
        "            args.epochs * len(train_loader),\n",
        "            args.lr,  # lr_lambda computes multiplicative factor\n",
        "            1e-6))\n",
        "    trainer.set_optimizer(optimizer)\n",
        "    trainer.set_data_loader(loader_dict)\n",
        "\n",
        "    if os.path.exists(args.load_path) and args.resume:\n",
        "        trainer.load_model(args.save_dir+'state.pth')\n",
        "        trainer.net.load_state_dict(torch.load(args.load_path),strict=False)\n",
        "    #trainer.scheduler = None\n",
        "    #trainer.set_optimizer(optimizer)\n",
        "    \n",
        "    trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBd66kgT9W5e"
      },
      "source": [
        "#パラメータ設定\n",
        "import easydict\n",
        "def get_pretrain_args(main_path):\n",
        "    args = easydict.EasyDict({\n",
        "        'projection_dim':128, 'temperature':0.5, 'k':200,\n",
        "        'batch_size':1024, 'epochs':1000,\n",
        "\n",
        "        'lr':1e-6, 'weight_decay':1e-6,\n",
        "        \n",
        "        'seed':0,  'resume':True,\n",
        "        \n",
        "         'save_dir':main_path+'pretrain/',\n",
        "         'load_path':main_path+'pretrain/latest_model.pth'\n",
        "\n",
        "    })\n",
        "    return args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltTPfh7VhPHx"
      },
      "source": [
        "\"\"\"\n",
        "保存フォルダおよびパラメータの設定をして\n",
        "事前学習を実行\n",
        "実験が途切れて途中からでもok\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "main_path = '/content/drive/My Drive/CL3/'\n",
        "args = get_pretrain_args(main_path)\n",
        "#args.batch_size = 216\n",
        "print(datetime.datetime.now())\n",
        "CL_main(args)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "974ypQUhLKL2"
      },
      "source": [
        "##Train 'n' Eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu8NEnGTNOZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517e896e-d3b2-4215-a105-85aec1811a81"
      },
      "source": [
        "\"\"\"\n",
        "classifier の学習\n",
        "test_times:test dataを水増する倍率\n",
        "(test dataがラベル付きデータのため非常に少ないことを想定して)\n",
        "\"\"\"\n",
        "class FineTuning:\n",
        "    def __init__(self, net, num_classes,batch_size, epochs,save_dir, is_save,test_times):\n",
        "\n",
        "        super(FineTuning, self).__init__()\n",
        "\n",
        "        self.net = net\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.optimizer = None\n",
        "        self.loader_dict = {}\n",
        "\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.it = 0\n",
        "\n",
        "        self.is_save = is_save\n",
        "\n",
        "        self.test_times = 1\n",
        "        if test_times is not None:\n",
        "          self.test_times = test_times\n",
        "\n",
        "    def set_data_loader(self, loader_dict):\n",
        "        self.loader_dict =loader_dict\n",
        "\n",
        "    def set_optimizer(self, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def set_loss(self, loss=None):\n",
        "        if loss is None:\n",
        "            self.loss = nn.CrossEntropyLoss()\n",
        "        else:\n",
        "            self.loss = loss\n",
        "\n",
        "    def train_val(self, is_train):\n",
        "        if is_train:\n",
        "            self.net.train()\n",
        "            data_loader = self.loader_dict['train']\n",
        "        else:\n",
        "            self.net.eval()\n",
        "            data_loader = self.loader_dict['test']\n",
        "\n",
        "        total_loss, total_correct_1, total_num = 0.0, 0.0, 0\n",
        "        with (torch.enable_grad() if is_train else torch.no_grad()):\n",
        "            for data, target in data_loader:\n",
        "                data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "                out = self.net(data)\n",
        "                loss = self.loss(out, target)\n",
        "\n",
        "                if is_train:\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                total_num += data.size(0)\n",
        "                total_loss += loss.item() * data.size(0)\n",
        "                prediction = torch.argsort(out, dim=-1, descending=True)\n",
        "                total_correct_1 += torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "                \n",
        "        return total_loss / total_num, total_correct_1 / total_num\n",
        "\n",
        "    def train(self):\n",
        "        os.makedirs(self.save_dir,exist_ok=True)\n",
        "        results = {'train_loss': [], 'train_acc@1': [],\n",
        "                    'test_loss': [], 'test_acc@1': []}\n",
        "        best_acc = 0.0\n",
        "        start = self.it + 1\n",
        "        \n",
        "        for epoch in range(start, self.epochs + 1):\n",
        "            train_loss, train_acc_1 = self.train_val(is_train=True)\n",
        "            results['train_loss'].append(train_loss)\n",
        "            results['train_acc@1'].append(train_acc_1)\n",
        "            test_loss, test_acc_1 = 0,0\n",
        "            for _ in range(self.test_times):\n",
        "              t_loss,t_acc_1 = self.train_val(is_train=False)\n",
        "              test_loss += t_loss\n",
        "              test_acc_1 += t_acc_1\n",
        "            test_loss /= self.test_times\n",
        "            test_acc_1 /= self.test_times\n",
        "            results['test_loss'].append(test_loss)\n",
        "            results['test_acc@1'].append(test_acc_1)\n",
        "\n",
        "            with open('{}res.csv'.format(self.save_dir), 'a') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([epoch, train_loss, train_acc_1, test_loss, test_acc_1])\n",
        "            print('[{}]//Test Epoch: [{}/{}] TrainLoss:{:.2f} TrainAcc@1:{:.2f} TestLoss:{:.2f} TestAcc@1:{:.2f}'\n",
        "                                     .format(datetime.datetime.now(), epoch, self.epochs, \\\n",
        "                                             train_loss,train_acc_1, test_loss, test_acc_1))\n",
        "            self.it = epoch\n",
        "            if test_acc_1 >= best_acc:\n",
        "                best_acc = test_acc_1\n",
        "                if self.is_save is True:\n",
        "                    self.save_classifier('best_fc_model.pth', os.path.join(self.save_dir))\n",
        "\n",
        "        return results['train_acc@1'][-1], best_acc, test_acc_1\n",
        "\n",
        "    def save_classifier(self, save_name, save_path):\n",
        "        save_filename = os.path.join(save_path, save_name)\n",
        "\n",
        "        torch.save({'classifier': self.net.fc.state_dict(),\n",
        "                    'optimizer': self.optimizer.state_dict(),#'scheduler': self.scheduler.state_dict(),\n",
        "                    'it': self.it}, save_filename)\n",
        "    \n",
        "    \n",
        "    def load_classifer(self, load_path):\n",
        "        checkpoint = torch.load(load_path)\n",
        "\n",
        "        for key in checkpoint.keys():\n",
        "            if 'classifier' in key:\n",
        "                self.net.fc.load_state_dict(checkpoint[key], strict=False)\n",
        "            elif key == 'it':\n",
        "                #self.it = 0\n",
        "                self.it = checkpoint[key]\n",
        "            elif key == 'optimizer':\n",
        "                self.optimizer.load_state_dict(checkpoint[key])\n",
        "            else:\n",
        "                getattr(self, key).load_state_dict(checkpoint[key])\n",
        "            print(f\"Check Point Loading: {key} is LOADED\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n    train_data = CIFAR10(root='data', train=True, transform=train_transform, download=True)\\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\\n    test_data = CIFAR10(root='data', train=False, transform=test_transform, download=True)\\n    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\\n    loader_dict = {'train':train_loader, 'test':test_loader}\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeH2nyMtTx_N"
      },
      "source": [
        "#パラメータ設定\n",
        "import easydict\n",
        "def get_tuning_args(main_path):\n",
        "    args = easydict.EasyDict({\n",
        "        'batch_size':64, 'epochs':100,\n",
        "        'num_classes':10, 'dataset':'cifar10',\n",
        "\n",
        "        'lr':5e-3, 'weight_decay':1e-6,\n",
        "        \n",
        "        'seed':0,  'resume':False,\n",
        "        'is_save':False, 'is_ga':True,\n",
        "        'test_times':1,\n",
        "        \n",
        "        'index_dir':'none',\n",
        "        'data_dir':'./data',\n",
        "\n",
        "        'save_dir':main_path+'fineTuning/',\n",
        "        'load_encoder_path':main_path+'pretrain/latest_model.pth',\n",
        "        'load_fc_path':main_path+'fineTuning/best_fc_model.pth'\n",
        "\n",
        "    })\n",
        "    return args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwLc9YrgEweN"
      },
      "source": [
        "\"\"\"\n",
        "classifier学習の実行\n",
        "\"\"\"\n",
        "def tuning_main(args,lb_idx,val_idx,temp_idx = [],ind = [],dum_tar = []):\n",
        "\n",
        "    dum_idx = None\n",
        "    if len(dum_tar) is not 0:\n",
        "        dum_idx = np.load(args.index_dir + \"dummy.npy\")\n",
        "\n",
        "    if args.is_ga:\n",
        "        train_dset,eval_dset = creat_GADM_Dataset(train_index=lb_idx, temp_index=temp_idx,\n",
        "                                                temp_target=ind, eval_index=val_idx,\n",
        "                                                dum_index=dum_idx, dum_target=dum_tar,\n",
        "                                                name=args.dataset,num_classes=args.num_classes,\n",
        "                                                data_dir=args.data_dir)\n",
        "    else:\n",
        "        _train_dset = GA_Dataset(name=args.dataset, train=True, \n",
        "                             num_classes=args.num_classes, data_dir=args.data_dir)\n",
        "        train_dset = _train_dset.get_dset()\n",
        "\n",
        "    if args.use_test:\n",
        "        _eval_dset = GA_Dataset(name=args.dataset, train=False, \n",
        "                             num_classes=args.num_classes, data_dir=args.data_dir)\n",
        "        eval_dset = _eval_dset.get_dset()\n",
        "\n",
        "    train_loader = DataLoader(train_dset, batch_size=args.batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
        "    test_loader = DataLoader(eval_dset, batch_size=args.batch_size, shuffle=False, num_workers=1, pin_memory=True)\n",
        "    loader_dict = {'train':train_loader, 'test':test_loader}\n",
        "\n",
        "    model = Net(num_classes=args.num_classes, pretrained_path=args.load_encoder_path).cuda()\n",
        "    for param in model.f.parameters():\n",
        "        param.requires_grad = False\n",
        "  \n",
        "    trainer = FineTuning(model,\n",
        "                         args.num_classes,\n",
        "                         args.batch_size,\n",
        "                         args.epochs,\n",
        "                         args.save_dir,\n",
        "                         args.is_save,\n",
        "                         args.test_times)\n",
        "    \n",
        "    optimizer = RAdam(params=model.parameters(), lr=args.lr)\n",
        "    trainer.set_optimizer(optimizer)\n",
        "    trainer.set_loss()\n",
        "    trainer.set_data_loader(loader_dict)\n",
        "\n",
        "    if os.path.exists(args.load_fc_path) and args.resume:\n",
        "        trainer.load_model(args.load_fc_path)\n",
        "    \n",
        "    train_acc, test_acc, last_acc = trainer.train()\n",
        "    \n",
        "    return train_acc,test_acc, last_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5QSn1GEiqpV"
      },
      "source": [
        "\"\"\"\n",
        "あるデータに対する予測\n",
        "\"\"\"\n",
        "def eval_(args,index_):\n",
        "    checkpoint = torch.load(args.load_fc_path)\n",
        "    net = Net(num_classes=args.num_classes, pretrained_path=args.load_encoder_path).cuda()\n",
        "    \n",
        "    net.fc.load_state_dict(checkpoint['classifier'])\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        net.cuda()\n",
        "    net.eval()\n",
        "\n",
        "    val_idx = index_\n",
        "    temp_idx = np.load(args.index_dir + \"temp.npy\")\n",
        "\n",
        "    dset = getattr(torchvision.datasets, \"CIFAR10\")\n",
        "    dset = dset(args.data_dir, train=True, download=False)\n",
        "    data, targets = np.array(dset.data), np.array(dset.targets)\n",
        "\n",
        "    ev_data, evs, ev_idx = sample_labeled_data(data, targets, \n",
        "                                               len(val_idx), args.num_classes, val_idx)\n",
        "    te_data, tes, te_idx = sample_labeled_data(data, targets, \n",
        "                                               len(temp_idx), args.num_classes, temp_idx)\n",
        "    \n",
        "    #eval_trans = get_transform(mean[\"cifar10\"], std[\"cifar10\"], train=False)\n",
        "    ev_dset = BasicDataset(ev_data, evs.tolist(), args.num_classes, test_transform, \n",
        "                              False, None, False)\n",
        "    te_dset = BasicDataset(te_data, tes.tolist(), args.num_classes, test_transform, \n",
        "                              False, None, False)\n",
        "    eval_loader = DataLoader(ev_dset, batch_size=args.batch_size,\n",
        "                              shuffle=False, num_workers=1, pin_memory=True)\n",
        "    temp_loader = DataLoader(te_dset, batch_size=args.batch_size,\n",
        "                              shuffle=False, num_workers=1, pin_memory=True)\n",
        " \n",
        "    acc = 0.0\n",
        "    ind = []\n",
        "    ind2 = []\n",
        "    device2 = torch.device('cpu')\n",
        "    with torch.no_grad():\n",
        "        for image, target in eval_loader:\n",
        "            image = image.type(torch.FloatTensor).cuda()\n",
        "            logits = net(image)\n",
        "            acc += logits.cpu().max(1)[1].eq(target).sum().numpy()\n",
        "\n",
        "        for image, target in temp_loader:\n",
        "            image = image.type(torch.FloatTensor).cuda()\n",
        "            logits = net(image)\n",
        "            pseudo_label = torch.softmax(logits, dim=-1)\n",
        "            max_probs, max_idx = torch.max(pseudo_label, dim=-1)\n",
        "            p_label = max_idx.to(device2).detach().clone().numpy().tolist()\n",
        "            ind += p_label\n",
        "            ind2 += pseudo_label.to(device2).detach().clone().numpy().tolist()\n",
        "    \n",
        "    return acc/len(ev_dset),ind, ind2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0PHKtHcjURY"
      },
      "source": [
        "#GA Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WGAh3rcjxZA"
      },
      "source": [
        "##GA Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzFTTC4oHeBB"
      },
      "source": [
        "#個体学習のパラメータ設定\n",
        "import easydict\n",
        "def get_args(main_path,g,i):\n",
        "    args = get_tuning_args(main_path)\n",
        "\n",
        "    args.save_dir = main_path + \"{:03}gene/fix/{:03}/\".format(g,i)\n",
        "    args.index_dir = main_path + 'data_index/'\n",
        "    args.use_test = False\n",
        "\n",
        "    return args\n",
        "\n",
        "def get_eval_args(main_path):\n",
        "    args = easydict.EasyDict({\n",
        "        'load_path':main_path + 'base/' + 'fixmatch/model_best.pth',\n",
        "        'use_train_model':'store_true',\n",
        "\n",
        "        'net':'WideResNet','net_from_name':False,'depth':16,\n",
        "        'widen_factor':2,'leaky_slope':0.1,'dropout':0.0,\n",
        "\n",
        "        'batch_size':100,'data_dir':'./data','dataset':'cifar10',\n",
        "        'num_classes':10,'index_dir':main_path + 'data_index/'\n",
        "\n",
        "    })\n",
        "    return args\n",
        "\n",
        "\n",
        "IND_SIZE = 100                    #遺伝子数(ラベルを探索するデータ数)\n",
        "GENERATION_SIZE = 50              #1世代あたりの個体数\n",
        "#max_ = 10                        #ラベル数\n",
        "#max_Magnitude = max_ - 1         #遺伝子の最大値\n",
        "\n",
        "LABELED_SIZE = 0                  #ラベル付きデータ数(適応度計算時にtrainとして使用)\n",
        "VALIDATION_SIZE = 50              #ラベル付きデータ数(適応度計算時にtestとして使用)\n",
        "DUM_SIZE = 200                    #ダミーデータ数\n",
        "\n",
        "gene_his = 'gene_his.csv'         #適応度保存ファイル名\n",
        "pre_path = 'fineTuning/'            #classifierの事前学習のフォルダ名\n",
        "\n",
        "#交叉率,個体ごとの突然変異率,遺伝子座ごとの突然変異率,世代数\n",
        "CXPB, MUTPB, MUTPB2, NGEN = 0.9, 0.5, 0.1, 250\n",
        "num_trial = 1                     #適応度計算回数\n",
        "elite_num =2                      #エリート個体数\n",
        "\n",
        "#TDGAパラメータ(最終項目参照)\n",
        "t_init = 9e-3\n",
        "t_fin = 1e-5\n",
        "eps = 1e-7\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvQNDCtpnRaL"
      },
      "source": [
        "\"\"\"\n",
        "データの分割\n",
        "ラベル付きデータ/ラベルを探索するデータ/テストデータ\n",
        "のインデックスをnumpyで保存\n",
        "\"\"\"\n",
        "def split_GAdata(main_path,num_lb,num_val,num_prov):\n",
        "    dset = getattr(torchvision.datasets, \"cifar10\".upper())\n",
        "    dset = dset(\"./data\", train=True)\n",
        "    data, targets = dset.data, dset.targets\n",
        "    data, targets = np.array(data), np.array(targets)\n",
        "\n",
        "    #使用データのサンプリング\n",
        "    tmp_data,tmp_s,tmp_idx = sample_labeled_data(data, targets, \n",
        "                         num_labels= num_lb + num_val + num_prov ,num_classes=10,\n",
        "                         index=None,isUniform = True)\n",
        "    \n",
        "    #ラベル探索データとラベル付きデータの分割\n",
        "    prov_data,prov_s,prov_idx = sample_labeled_data(data[tmp_idx], targets[tmp_idx], \n",
        "                         num_labels= num_prov ,num_classes=10,\n",
        "                         index=None, isUniform=True)\n",
        "    lb_all_idx = np.array(sorted(list(set(range(len(tmp_data))) - set(prov_idx))))\n",
        "    prov_idx = tmp_idx[prov_idx.tolist()]\n",
        "    lb_all_idx = tmp_idx[lb_all_idx.tolist()]\n",
        "\n",
        "    #trainに組み込むものとvalidに使用するものに分割\n",
        "    lb_data,lb_s,lb_idx = sample_labeled_data(data[lb_all_idx], targets[lb_all_idx], \n",
        "                         num_labels= num_lb ,num_classes=10,\n",
        "                         index=None,isUniform=True)\n",
        "    val_idx = np.array(sorted(list(set(range(len(lb_all_idx))) - set(lb_idx))))\n",
        "    lb_idx = lb_all_idx[lb_idx.tolist()]\n",
        "    val_idx = lb_all_idx[val_idx.tolist()]\n",
        "\n",
        "    os.makedirs(main_path+\"data_index\",exist_ok=True)\n",
        "    np.save(main_path+\"data_index/lb\",lb_idx)\n",
        "    np.save(main_path+\"data_index/lb_init\",lb_idx)\n",
        "    np.save(main_path+\"data_index/val\",val_idx)\n",
        "    np.save(main_path+\"data_index/temp\",prov_idx)\n",
        "    np.save(main_path+\"data_index/temp_all\",prov_idx)\n",
        "    np.save(main_path+\"data_index/lb_all\",lb_all_idx)\n",
        "    np.save(main_path+\"data_index/correct\",prov_s)\n",
        "    #return tmp_idx,tmp_s,prov_idx,prov_s,lb_idx,lb_s,val_idx#,val_s\n",
        "\n",
        "\n",
        "#上記に加えダミーデータも分割するもの\n",
        "def split_GADMdata(main_path,num_lb,num_val,num_prov,num_dum):\n",
        "    dset = getattr(torchvision.datasets, \"cifar10\".upper())\n",
        "    dset = dset(\"./data\", train=True)\n",
        "    data, targets = dset.data, dset.targets\n",
        "    data, targets = np.array(data), np.array(targets)\n",
        "\n",
        "    #使用データのサンプリング\n",
        "    tmp_data,tmp_s,tmp_idx = sample_labeled_data(data, targets, \n",
        "                         num_labels= num_lb + num_val + num_prov + num_dum ,num_classes=10,index=None)\n",
        "    \n",
        "    #ダミー用データのサンプリング\n",
        "    ul_data,ul_s,ul_idx = sample_labeled_data(data[tmp_idx], targets[tmp_idx], \n",
        "                         num_labels= num_prov + num_dum ,num_classes=10,index=None)\n",
        "    lb_all_idx = tmp_idx[sorted(list(set(range(len(tmp_idx))) - set(ul_idx)))]\n",
        "    ul_idx = tmp_idx[ul_idx.tolist()]\n",
        "\n",
        "    #以下同様\n",
        "    prov_data,prov_s,prov_idx = sample_labeled_data(data[ul_idx], targets[ul_idx], \n",
        "                         num_labels= num_prov  ,num_classes=10,index=None)\n",
        "    dum_idx = ul_idx[list(set(range(len(ul_idx))) - set(prov_idx))]\n",
        "    prov_idx = ul_idx[prov_idx.tolist()]\n",
        "    \n",
        "    lb_data,lb_s,lb_idx = sample_labeled_data(data[lb_all_idx], targets[lb_all_idx], \n",
        "                         num_labels= num_lb ,num_classes=10,index=None)\n",
        "    val_idx = lb_all_idx[sorted(list(set(range(len(lb_all_idx))) - set(lb_idx)))]\n",
        "    lb_idx = lb_all_idx[lb_idx.tolist()]\n",
        "\n",
        "    os.makedirs(main_path+\"data_index\",exist_ok=True)\n",
        "    np.save(main_path+\"data_index/lb\",lb_idx)\n",
        "    np.save(main_path+\"data_index/val\",val_idx)\n",
        "    np.save(main_path+\"data_index/temp\",prov_idx)\n",
        "    np.save(main_path+\"data_index/temp_all\",prov_idx)\n",
        "    np.save(main_path+\"data_index/lb_all\",lb_all_idx)\n",
        "    np.save(main_path+\"data_index/correct\",prov_s)\n",
        "    np.save(main_path+\"data_index/dummy\",dum_idx)\n",
        "    #return tmp_idx,tmp_s,prov_idx,prov_s,lb_idx,lb_s,val_idx#,val_s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqB_gg5rhchh"
      },
      "source": [
        "#突然変異(個体内)\n",
        "def mutate(individual,indpb, para_len):\n",
        "  for i,_max in enumerate(para_len):\n",
        "    if random.random() < indpb:\n",
        "      temp = individual[i]\n",
        "      individual[i] += random.randint(0,_max-1)\n",
        "      individual[i] += 1\n",
        "      individual[i] %= _max\n",
        "  return individual,\n",
        " \n",
        "#交叉\n",
        "def mate(individuals, prob=1):\n",
        "  for child1, child2 in zip(individuals[::2], individuals[1::2]):\n",
        "      if random.random() < prob:\n",
        "        tools.cxUniform(child1, child2, 0.5)\n",
        "\n",
        "\n",
        "#突然変異　　mutpb:個体ごとの確率,mutpb2:遺伝子座ごとの確率\n",
        "def mutation(offspring, mutpb, mutpb2,para_len):\n",
        "  for mutant in offspring:\n",
        "      if random.random() < mutpb:\n",
        "        mutate(mutant, mutpb2,para_len)\n",
        "\n",
        "\n",
        "#適応度計算\n",
        "def one_eval(main_path,g,i,ind,seeds,dum):\n",
        "  with open(main_path + \"progress.csv\", 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([g,i,\"running\"])\n",
        "\n",
        "  #パラメータ再設定(別でまとめてもいいかも)\n",
        "  args = get_args(main_path,g,i)\n",
        "  args.resume = False\n",
        "  args.lr =1e-2\n",
        "  args.epochs = 20\n",
        "  args.batch_size = 32\n",
        "  args.test_times = 3\n",
        "  fitness = []\n",
        "  train_acc = []\n",
        "\n",
        "  #必要インデックスの取得\n",
        "  lb_idx = np.load(args.index_dir + \"lb.npy\")\n",
        "  val_idx = np.load(args.index_dir + \"val.npy\")\n",
        "  prov_idx = np.load(args.index_dir+\"temp.npy\")\n",
        "  para_index = np.load(args.index_dir + \"para.npy\", allow_pickle=True)\n",
        "\n",
        "  #個体をラベル集合に変換\n",
        "  para_ind = [i[j] for i,j in zip(para_index,ind)]\n",
        "\n",
        "  args.load_encoder_path = main_path + \"pretrain/latest_model.pth\"\n",
        "\n",
        "  #適応度計算\n",
        "  for j,seed in enumerate(seeds):\n",
        "      args.seed = seed\n",
        "      args.save_dir = main_path + \"{:03}gene/fix/{:03}/{:03}/\".format(g,i,j)\n",
        "      os.makedirs(args.save_dir,exist_ok=True)\n",
        "      train_ac, test_ac,last_ac = tuning_main(args, lb_idx, val_idx, prov_idx, para_ind, dum)\n",
        "      fitness.append((test_ac+last_ac)/2) #過学習および学習不足の差異を減らすため\n",
        "      train_acc.append(train_ac)\n",
        "\n",
        "\n",
        "  total_fitness = np.average(fitness) \n",
        "  correct = np.load(args.index_dir + \"correct.npy\")\n",
        "  real_acc = accuracy_score(correct,para_ind) \n",
        "  print(\"{:03}gene/{:03}ind::acc:{:03f}\".format(g,i,total_fitness))\n",
        "  del ind.fitness.values\n",
        "  ind.fitness.values = total_fitness,\n",
        "\n",
        "  with open(main_path +\"{:03}gene/\".format(g)+ gene_his,'a') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([total_fitness, real_acc]+fitness+train_acc)\n",
        "  with open(main_path + \"progress.csv\", 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([g,i,\"fin\"])\n",
        "\n",
        "#個体生成\n",
        "def creat_pop(main_path):\n",
        "\n",
        "  #encoerがない場合生成\n",
        "  if not os.path.exists(main_path+\"pretrain/\")::\n",
        "      args = get_pretrain_args(main_path)\n",
        "      args.batch_size = 216\n",
        "      print(datetime.datetime.now())\n",
        "      CL_main(args)\n",
        "\n",
        "  #事前学習\n",
        "  args = get_args(main_path,0,0)\n",
        "  args.save_dir = main_path + pre_path\n",
        "  args.is_save = True\n",
        "  args.use_test = True\n",
        "  args.ga_ = False\n",
        "  lb_idx = np.load(args.index_dir + \"lb_all.npy\")\n",
        "  _,__,___ = tuning_main(args, lb_idx, [], [], [], [])\n",
        "\n",
        "  args.load_fc_path = main_path + pre_path + \"-1/best_fc_model.pth\"\n",
        "  val_idx = np.load(args.index_dir + \"val.npy\")\n",
        "  val_acc,_,pre_mat = eval_(args, val_idx )\n",
        "  \n",
        "  #予測値の低いラベルの足切り\n",
        "  y = np.where(np.array(pre_mat) > 0.075)\n",
        "  z = [[] for _ in range(IND_SIZE)]\n",
        "  for y0,y1 in zip(y[0],y[1]):\n",
        "    z[y0].append(y1)\n",
        "  np.save(args.index_dir + 'para.npy', z)\n",
        "  zz = np.array([len(i) for i in z])\n",
        "  np.save(args.index_dir + 'para_len.npy',zz)\n",
        "\n",
        "  cor_cnt = 0\n",
        "  correct = np.load(args.index_dir + \"correct.npy\")\n",
        "  for k,tar in zip(correct,z):\n",
        "    if np.any(tar == k):\n",
        "      cor_cnt += 1\n",
        "  print(cor_cnt)\n",
        "  print(np.average(zz),zz)\n",
        "\n",
        " #初期個体の生成\n",
        "  creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "  creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "  toolbox = base.Toolbox()\n",
        "\n",
        "  pop = []\n",
        "  for i in range(GENERATION_SIZE):\n",
        "    base_ind = [random.randrange(i) for i in zz]\n",
        "    ind = creator.Individual(copy.deepcopy(base_ind))\n",
        "    #mutate(ind,ev_miss)\n",
        "    pop.append(ind)\n",
        "  return pop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1FPeGj4S6sv"
      },
      "source": [
        "#ラベル付きデータをtrainとvalidに分割(データ数が少ないため現在未使用)\n",
        "def split_lb(main_path,num_train):\n",
        "  index_dir = main_path + \"data_index/\"\n",
        "  lb_all_idx = np.load(index_dir + \"lb_all.npy\")\n",
        "  dset = getattr(torchvision.datasets,\"CIFAR10\")\n",
        "  dset = dset(\"./data\", train=True)\n",
        "  data, targets = np.array(dset.data), np.array(dset.targets)\n",
        "\n",
        "  lb_all_data, lb_all_s, lb_all_idx = sample_labeled_data(data, targets,\n",
        "                                               num_labels=len(lb_all_idx), num_classes=10, index=lb_all_idx)\n",
        "  lb_data, lbs, lb_idx = sample_labeled_data(data[lb_all_idx], targets[lb_all_idx],\n",
        "                                               num_labels=num_train, num_classes=10, index=None)\n",
        "  val_idx = np.array(sorted(list(set(range(len(lb_all_idx))) - set(lb_idx))))\n",
        "  lb_idx = lb_all_idx[lb_idx.tolist()]\n",
        "  val_idx = lb_all_idx[val_idx.tolist()]\n",
        "  \n",
        "  np.save(index_dir+\"lb\",lb_idx) \n",
        "  np.save(index_dir+\"val\",val_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ch9VUqBj4oA"
      },
      "source": [
        "##Main exe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmB_oUI-5J02"
      },
      "source": [
        "#GA本体\n",
        "def main_GA():\n",
        "\n",
        "  #現在の学習経過の取得::gene_cnt:世代数,num_fin_ind:探索中個体番号\n",
        "  gene_cnt,num_fin_ind = 0,0\n",
        "  isRunning = \"\"\n",
        "  if  os.path.exists(main_path + \"progress.csv\")==True:\n",
        "    with open(main_path + \"progress.csv\", 'r') as f:\n",
        "      for i,row in enumerate(csv.reader(f, lineterminator='\\n')):\n",
        "        gene_cnt = int(row[0])\n",
        "        num_fin_ind = int(row[1])\n",
        "        isRunning = row[2]\n",
        "  else:\n",
        "    with open(main_path + \"progress.csv\",'w') as f:\n",
        "      f.write(\"\")\n",
        "\n",
        "  if isRunning == \"running\":\n",
        "    if os.path.exists(main_path + \"{:03}gene/fix/{:03}\".format(gene_cnt,num_fin_ind)):\n",
        "      shutil.rmtree(main_path + \"{:03}gene/fix/{:03}\".format(gene_cnt,num_fin_ind))\n",
        "\n",
        "  \n",
        "  #個体設定\n",
        "  creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "  creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        " \n",
        "  toolbox = base.Toolbox()\n",
        " \n",
        "  #toolbox.register(\"attribute\", random.randint, 0, max_Magnitude)\n",
        "  #toolbox.register(\"individual\", tools.initRepeat, creator.Individual,\n",
        "  #                 toolbox.attribute, IND_SIZE)\n",
        "  #toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "  #toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "  #toolbox.register(\"mutate\", tools.mutShuffleIndexes, indpb=0.1)\n",
        "  toolbox.register(\"select\", tools.selTournament, tournsize=2)\n",
        " \n",
        " \n",
        "  pop = []        #全個体のlist\n",
        "  fitnesses = []  #適応度のlist\n",
        "  #test_acc_lis = []\n",
        "\n",
        "  if gene_cnt == 0 and num_fin_ind == 0 and isRunning!=\"running\":\n",
        "\n",
        "    #初期個体生成\n",
        "    if not os.path.exists(main_path+\"data_index/\"):\n",
        "        split_GADMdata(main_path,LABELED_SIZE,VALIDATION_SIZE,IND_SIZE,DUM_SIZE)\n",
        "    #pop = toolbox.population(n=GENERATION_SIZE)\n",
        "    pop = creat_pop(main_path)\n",
        "    os.makedirs(main_path+\"{:03}gene/\".format(gene_cnt),exist_ok=True)\n",
        "    os.makedirs(main_path+\"{:03}gene/fix/\".format(gene_cnt),exist_ok=True)\n",
        "\n",
        "    _ind = np.empty((0,IND_SIZE), int)\n",
        "    for ind in pop:\n",
        "        _ind = np.append(_ind, np.array([ind]),axis=0)\n",
        "    np.save(main_path+\"{:03}gene/ind\".format(gene_cnt),_ind)\n",
        "\n",
        "    seed = np.array(random.sample(range(2**31 -1),num_trial))\n",
        "    np.save(main_path+\"{:03}gene/seed\".format(gene_cnt),seed)\n",
        "\n",
        "    #ダミーのラベル生成\n",
        "    dum_tar = np.array(random.sample([i%10 for i in range(DUM_SIZE)],DUM_SIZE))\n",
        "    np.save(main_path+\"{:03}gene/dum\".format(gene_cnt),dum_tar)\n",
        "\n",
        "    #適応度計算\n",
        "    for i,ind in enumerate(pop):\n",
        "        one_eval(main_path,gene_cnt,i,ind,seed,dum_tar)\n",
        "\n",
        "  elif isRunning==\"running\":\n",
        "      #現世代の個体取得\n",
        "      _ind = np.load(main_path + \"{:03}gene/ind.npy\".format(gene_cnt))\n",
        "      for ind_ in _ind:\n",
        "          ind = creator.Individual(ind_.tolist())\n",
        "          pop.append(ind)\n",
        "\n",
        "    #計算済みの適応度取得\n",
        "    acc_lis = []\n",
        "    if os.path.exists(main_path +\"{:03}gene/\".format(gene_cnt)+ gene_his):\n",
        "        with open(main_path +\"{:03}gene/\".format(gene_cnt)+ gene_his,'r') as f:\n",
        "            for row in csv.reader(f,lineterminator='\\n'):\n",
        "                acc_lis.append(float(row[0]))\n",
        "\n",
        "    seed = np.load(main_path+\"{:03}gene/seed.npy\".format(gene_cnt))\n",
        "    dum_tar = np.load(main_path+\"{:03}gene/dum.npy\".format(gene_cnt))\n",
        "\n",
        "    #適応度計算および個体と紐づけ\n",
        "    for i,ind in  enumerate(pop):\n",
        "        if i < num_fin_ind:\n",
        "          ind.fitness.values = acc_lis[i],\n",
        "        else:\n",
        "          one_eval(main_path,gene_cnt,i,ind,seed,dum_tar)\n",
        "\n",
        "  g = gene_cnt\n",
        "\n",
        "  for gene_cnt in range(g + 1, NGEN):\n",
        " \n",
        "    print(\"***generation{}*****************\".format(gene_cnt))\n",
        "    #次世代の個体生成\n",
        "    elite = tools.selBest(pop, k= elite_num)\n",
        "    offspring = toolbox.select(pop, len(pop)- elite_num)\n",
        "    offspring = list(map(toolbox.clone, offspring))\n",
        "    mate(offspring)\n",
        "    para_len = np.load(main_path + \"data_index/para_len.npy\")\n",
        "    mutation(offspring, MUTPB, MUTPB2, para_len)\n",
        "    offspring.extend(elite)\n",
        "    pop[:] = offspring\n",
        "    \n",
        "    os.makedirs(main_path+\"{:03}gene/\".format(gene_cnt),exist_ok=True)\n",
        "    os.makedirs(main_path+\"{:03}gene/fix/\".format(gene_cnt),exist_ok=True)\n",
        "    \n",
        "    _ind = np.empty((0,IND_SIZE), int)\n",
        "    for ind in pop:\n",
        "        _ind = np.append(_ind, np.array([ind]),axis=0)\n",
        "    np.save(main_path+\"{:03}gene/ind\".format(gene_cnt),_ind)\n",
        "\n",
        "    seed = np.array(random.sample(range(2**31 -1),num_trial))\n",
        "    np.save(main_path+\"{:03}gene/seed\".format(gene_cnt),seed)\n",
        "\n",
        "    dum_tar = np.array(random.sample([i%10 for i in range(DUM_SIZE)],DUM_SIZE))\n",
        "    np.save(main_path+\"{:03}gene/dum\".format(gene_cnt),dum_tar)\n",
        "    \n",
        "    #適応度計算\n",
        "    for i,ind in enumerate(pop):\n",
        "        one_eval(main_path,gene_cnt,i,ind,seed,dum_tar)\n",
        "\n",
        "  return pop\n",
        " \n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "  main_path = \"/content/drive/My Drive/CLGA/\"\n",
        "  os.makedirs(main_path,exist_ok=True)\n",
        "  print(main_GA())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BMvyV3usOqX"
      },
      "source": [
        "\"\"\"\n",
        "path = '/content/drive/My Drive/CLGA/'\n",
        "with open(path + \"progress.csv\", 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([0,0,\"running\"])\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5rZAmd9s8OJ"
      },
      "source": [
        "#Research"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emLzdhAytDGe"
      },
      "source": [
        "\"\"\"\n",
        "個体集計\n",
        "genes に二次配列で個体を取得\n",
        "\"\"\"\n",
        "main_path = \"/content/drive/My Drive/CLGA/\"\n",
        "correct = np.load(main_path + \"data_index/correct.npy\").tolist()\n",
        "para_index = np.load(args.index_dir + \"para.npy\", allow_pickle=True).tolist()\n",
        "genes = []\n",
        "gene_tmp = []\n",
        "with open(main_path + \"progress.csv\", 'r') as f:\n",
        "    for i,row in enumerate(csv.reader(f, lineterminator='\\n')):\n",
        "        gene_cnt = int(row[0])\n",
        "        num_fin_ind = int(row[1])\n",
        "        isRunning = row[2]\n",
        "num_gene = gene_cnt - 1\n",
        "\n",
        "for i in range(num_gene):\n",
        "  gene_tmp = []\n",
        "  x = np.load(main_path + \"{:03}gene/ind.npy\".format(i)).tolist()\n",
        "  x = [i[j] for i,j in zip(para_index,x)]\n",
        "  genes.append(x)\n",
        "  print(\"\\r{}/{}\".format(i,num_gene),end=\"\")\n",
        "  time.sleep(.10)\n",
        "\n",
        "print(\"\")\n",
        "num_ind = len(genes[0][0])\n",
        "print(num_gene, num_ind)\n",
        "print(genes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELEkimksiHCO"
      },
      "source": [
        "\"\"\"\n",
        "x :　個体の適応度\n",
        "y :  個体の実際の精度\n",
        "\"\"\"\n",
        "\n",
        "gene_his = \"gene_his.csv\"\n",
        "x = []\n",
        "y = []\n",
        "for i in  range(num_gene):\n",
        "  tmp1 = []\n",
        "  tmp2 = []\n",
        "  with open(main_path +\"{:03}gene/\".format(i)+ gene_his,'r') as f:\n",
        "    for row in csv.reader(f,lineterminator='\\n'):\n",
        "      tmp1.append(float(row[0]))\n",
        "      tmp2.append(float(row[1]))\n",
        "  x.append(tmp1)\n",
        "  y.append(tmp2)\n",
        "  print(\"\\r{}/{}\".format(i,num_gene),end=\"\")\n",
        "\n",
        "print(\"\")\n",
        "max_index = [i.index(max(i)) for i in x]\n",
        "fit_max = [i[j] for i,j in zip(x,max_index)]\n",
        "corret_for_fitmax = [i[j] for i,j in zip(y,max_index)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVRtWhOgiVpP"
      },
      "source": [
        "def ensemble(targets,num_labels,bias=None):\n",
        "  targets = np.array(targets)\n",
        "  num = targets.shape[1]\n",
        "  if bias==None:\n",
        "    bias = [1.0 for _ in range(num)]\n",
        "  ans_mat = np.array([[0.0 for __ in  range(num_labels)] for _ in  range(num)])\n",
        "  for j,target in enumerate(targets):\n",
        "    for i,x in enumerate(target):\n",
        "      ans_mat[i][int(x)] += bias[j]\n",
        "  ans = [np.argmax(x) for x in ans_mat]\n",
        "  return ans\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFxdFgUciYd3"
      },
      "source": [
        "\"\"\"\n",
        "acc_lis : 個体の適応度\n",
        "ens_acc_lis : 1世代におけるあるデータにおける\n",
        "              もっとも採択されたラベルに対する精度\n",
        "\"\"\"\n",
        "\n",
        "acc_lis = []\n",
        "ens_acc_lis = []\n",
        "for ones in genes:\n",
        "  tmp=[]\n",
        "  for ind in ones:\n",
        "    cnt = 0\n",
        "    for i,j in zip(ind,correct):\n",
        "      if i==j:\n",
        "        cnt += 1\n",
        "    tmp.append(cnt)\n",
        "  ens = ensemble(ones,10)\n",
        "  acc = accuracy_score(ens,correct)\n",
        "  ens_acc_lis.append(acc)\n",
        "  acc_lis.append(tmp)\n",
        "\n",
        "print(ens_acc_lis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiwjT7ZFinzt"
      },
      "source": [
        "#散布図\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "x_ = []\n",
        "y_ = []\n",
        "num1 = 0\n",
        "rang = 10000\n",
        "for xx,yy in zip(x,y):\n",
        "  x_.extend(xx)\n",
        "  y_.extend(yy)\n",
        "print(len(x_))\n",
        "print(np.corrcoef(x_,y_)[0][1])\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.scatter(y_[num1:num1 + rang],x_[num1:num1 + rang])\n",
        "\n",
        "plt.ylabel(\"fitness value\" ,fontsize=15)\n",
        "plt.xlabel(\"correct num\", fontsize=15)\n",
        "plt.grid(True)\n",
        "fig.savefig(\"img.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MoIua2AiwDF"
      },
      "source": [
        "#世代に対する推移\n",
        "\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from statistics import mean\n",
        "\n",
        "max_acc = []\n",
        "acc_lis = []\n",
        "\n",
        "max_acc = [ max(x_) for x_ in y]\n",
        "ave_acc = [mean(li) for li in y]\n",
        "ca_min = min(ave_acc) - 0.05\n",
        "ca_max = max(max_acc) + 0.05\n",
        "\n",
        "acc_lis = tuple(x)\n",
        "al_max = max([max(i) for i in acc_lis]) + 0.05\n",
        "al_min = min([min(i) for i in  acc_lis]) - 0.05\n",
        "#print(max_acc)\n",
        "fig, ax = plt.subplots(figsize=(10,5),dpi=100)\n",
        "ax2 = ax.twinx()\n",
        "num = 20\n",
        "\n",
        "\n",
        "#fig = plt.figure(figsize=(10,10),dpi=200)\n",
        "\n",
        "bp = ax.boxplot(acc_lis, positions=[i for i in range(num_gene)])\n",
        "#ax.set_xticklabels([str(i) for i in range(1,len(acc_lis)+1)])\n",
        "ax.set_ylim([al_min , al_max ])\n",
        "\n",
        "ax2.set_xticks(np.arange(-1, num_gene, 10))\n",
        "ax2.set_xticklabels([str(i) for i in range(0,num_gene,10)])\n",
        "ax2.set_xticks(np.arange(-1, num, 10.0),minor=True)\n",
        "ax2.set_ylim([ca_min, ca_max])\n",
        "\n",
        "ax2.plot(max_acc,marker='.', label='best correct rate')\n",
        "ax2.plot(ave_acc,marker='.', label='average correct rate')\n",
        "\n",
        "ax2.legend()\n",
        "\n",
        "plt.legend(loc='lower right', fontsize=15)\n",
        "plt.grid()\n",
        "ax.set_xlabel('generation', fontsize=15)\n",
        "ax2.set_ylabel('accuracy about individual', fontsize=15)\n",
        "ax.set_ylabel('fitness accuracy ', fontsize=15)\n",
        "\n",
        "plt.savefig(\"graph.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIF_ytsYSNaV"
      },
      "source": [
        "#SomeThing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGRaNdxzqGw8"
      },
      "source": [
        "#TDGA()\n",
        "    off_o = list(map(toolbox.clone, toolbox.select(pop, GENERATION_SIZE)))\n",
        "    off_p = list(map(toolbox.clone, pop))\n",
        "\n",
        "    mate(off_o)\n",
        "    para_len = np.load(main_path + 'data_index/para_len.npy')\n",
        "    mutation(off_o,MUTPB, MUTPB2, para_len)\n",
        "    mutation(off_p,MUTPB, MUTPB2, para_len)\n",
        "\n",
        "    off_d = list(map(toolbox.clone, elite))\n",
        "    off_d.extend(off_o)\n",
        "    off_d.extend(off_p)\n",
        "\n",
        "    tt = gene_cnt/ NGEN\n",
        "    temperature = pow(t_init, 1-tt)*pow(t_fin, tt)\n",
        "    def closure(pop_):\n",
        "      n = len(pop_) + 1\n",
        "      npop = np.array(pop_)\n",
        "      cnt_list = np.array([np.count_nonzero(npop == i, axis=0) for i in range(10)]).T.tolist()\n",
        "      prov_h = [[-x / n * math.log2(x / n + eps ) for x in lis]for lis in cnt_list]\n",
        "\n",
        "      prov_hall =sum([sum(x) for x in prov_h])\n",
        "      E_sum = sum([ind.fitness.values[0] for ind in pop_])\n",
        "      def compute_F(ind):\n",
        "        h = prov_hall\n",
        "        for i,gene in enumerate(ind):\n",
        "          h -= prov_h[i][gene]\n",
        "          num_g = cnt_list[i][gene] + 1\n",
        "          h += -num_g / n * math.log2(num_g / n)\n",
        "\n",
        "\n",
        "        F = (E_sum + ind.fitness.values[0])/n - h* temperature\n",
        "        return F\n",
        "      return compute_F \n",
        "\n",
        "    selected = elite\n",
        "    for i in range(GENERATION_SIZE - elite_num):\n",
        "\n",
        "      f = closure(selected)\n",
        "      Fs = [f(ind) for ind in off_d]\n",
        "      idx = Fs.index(min(Fs))\n",
        "      selected.append(off_d[idx])\n",
        "      off_d.pop(idx)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}